# LP理论基础和实践（进阶）task—02

#### 一、文本表示概念

文本表示的意思是把字词处理成向量或矩阵，以便计算机能进行处理。文本表示是自然语言处理的开始环节。

文本表示按照细粒度划分，一般可分为字级别、词语级别和句子级别的文本表示。

文本表示分为离散表示和分布式表示。离散表示的代表就是词袋模型，one-hot（也叫独热编码）、TF-IDF、n-gram都可以看作是词袋模型。分布式表示也叫做词嵌入（word embedding），经典模型是word2vec，还包括后来的Glove、ELMO、GPT和最近很火的BERT。

#### 二、词袋模型

（1）one-hot

第一个问题是数据稀疏和维度灾难。第二个问题是没有考虑句中字的顺序性。第三个问题是没有考虑字的相对重要性

（2）TF-IDF

TF-IDF用来评估字词对于文档集合中某一篇文档的重要程度。字词的重要性与它在某篇文档中出现的次数成正比，与它在所有文档中出现的次数成反比。TF-IDF的计算公式为：

 ![技术图片](http://image.mamicode.com/info/201903/20190316192358041214.png)

TF-IDF的思想比较简单，但是却非常实用。然而这种方法还是存在着数据稀疏的问题，也没有考虑字的前后信息。

##### （3）n-gram

这种表示方法的好处是可以获取更丰富的特征，提取字的前后信息，考虑了字之间的顺序性。

但是问题也是显而易见的，这种方法没有解决数据稀疏和词表维度过高的问题，而且随着n的增大，词表维度会变得更高。

#### 三、分布式表示

Word2vec是Google的Mikolov等人提出来的一种文本分布式表示的方法，这种方法是对神经网络语言模型的“瘦身”， 巧妙地运用层次softmax（hierarchical softmax ）和负采样（Negative sampling ）两种技巧，使得原本参数繁多、计算量巨大的神经网络语言模型变得容易计算。

Word2vec概括地说是包含了两种模型和两种加速训练方法：

（1）两种模型：CBOW（continuous bag-of-words）和Skip-Gram。CBOW的目标是通过上下文的词语预测中间的词是什么。而skip-gram则相反，由一个特定的词来预测前后可能出现的词。这两个模型并非是在Word2vec中首次提出，而是神经网络语言模型中就有的。

（2）两种方法：层次softmax和负采样。层次softmax是通过构建一种有效的树结构（哈夫曼树，huffman tree）来加速计算词语的概率分布的方法；而负采样则是通过随机抽取负样本，与正样本一起参加每次迭代，变成一个二分类问题而减少计算量的方法。

#### 四、word2vec原理

推荐两个简书连接，写的很详细。

（1）<https://www.jianshu.com/p/1405932293ea>

（2）<https://www.jianshu.com/p/d0e2d00fb4f0>



